_online supplement page for an article **Building style-aware neural MIDI synthesizers using simplified differentiable DSP approach**"_

## Abstract
We explore how simplified differentiable DSP approach can be used to build realistically sounding virtual MIDI-controllable monophonic synthesizers. The simplification involves directly using  MIDI data as input to the DDSP decoder. On top of that, we show how  incorporating additional style-based and temporal channels can be used to imitate various aspects of performance and improve realism.  We further demonstrate the results of applying the approach to the task of modelling the sound of electric guitar. The presented results were obtained with a model trained on less than 12 minutes of manually MIDI-annotated audio. The source code is released along with the prepared dataset.


The full text of the article is available [here](https://todo.com).

[here]: http://todo.com

## Audio Examples



## Source Code

the source code is available on GitHub in [neural-midi-synthesizer](https://github.com/hq9000/neural-midi-synthesizer) repository.

_note:_ the repository initially started as a clone of [ddsp_simplified](https://github.com/raraz15/ddsp_simplified).

## Dataset

the dataset can be downloaded from [here](https://todo1.com).

The dataset consists of:
- the audio in WAV format
- annotation as a MIDI file
- Reaper project containing both audio and MIDI